{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66c51841",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import os\n",
    "import tensorflow_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34a81754",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Warning: Setting shuffle_files=True because split=TRAIN and shuffle_files=None. This behavior will be deprecated on 2019-08-06, at which point shuffle_files=False will be the default for all splits.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfds.core.DatasetInfo(\n",
      "    name='ted_hrlr_translate',\n",
      "    version=0.0.1,\n",
      "    description='Data sets derived from TED talk transcripts for comparing similar language pairs\n",
      "where one is high resource and the other is low resource.\n",
      "',\n",
      "    urls=['https://github.com/neulab/word-embeddings-for-nmt'],\n",
      "    features=Translation({\n",
      "        'en': Text(shape=(), dtype=tf.string),\n",
      "        'pt': Text(shape=(), dtype=tf.string),\n",
      "    }),\n",
      "    total_num_examples=54781,\n",
      "    splits={\n",
      "        'test': 1803,\n",
      "        'train': 51785,\n",
      "        'validation': 1193,\n",
      "    },\n",
      "    supervised_keys=('pt', 'en'),\n",
      "    citation=\"\"\"@inproceedings{Ye2018WordEmbeddings,\n",
      "      author  = {Ye, Qi and Devendra, Sachan and Matthieu, Felix and Sarguna, Padmanabhan and Graham, Neubig},\n",
      "      title   = {When and Why are pre-trained word embeddings useful for Neural Machine Translation},\n",
      "      booktitle = {HLT-NAACL},\n",
      "      year    = {2018},\n",
      "      }\"\"\",\n",
      "    redistribution_info=,\n",
      ")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#导入数据\n",
    "examples, info = tensorflow_datasets.load('ted_hrlr_translate/pt_to_en', with_info=True,as_supervised=True)\n",
    "train_data, val_data = examples['train'], examples['validation']\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7986de07",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_tokenizer = tensorflow_datasets.features.text.SubwordTextEncoder.build_from_corpus((en.numpy() for en,pt in train_data),target_vocab_size= 2**13)\n",
    "pt_tokenizer = tensorflow_datasets.features.text.SubwordTextEncoder.build_from_corpus((pt.numpy() for en,pt in train_data),target_vocab_size= 2**13)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "97bbef26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#数据预处理\n",
    "buffer_size = 20000\n",
    "batch_size = 64\n",
    "max_length = 40\n",
    "\n",
    "def encode_to_subword(en_sentence, pt_sentence):#词汇表里有0 ~ vacab_size-1的序号，所以用vacab_size和vocab_size+1作为开始和结束的标识符\n",
    "    en_sequence =[en_tokenizer.vocab_size]\\\n",
    "    +en_tokenizer.encode(en_sentence.numpy())\\\n",
    "    +[en_tokenizer.vocab_size+1]\n",
    "    \n",
    "    pt_sequence =[pt_tokenizer.vocab_size]\\\n",
    "    +pt_tokenizer.encode(pt_sentence.numpy())\\\n",
    "    +[pt_tokenizer.vocab_size+1]\n",
    "    \n",
    "    return en_sequence, pt_sequence\n",
    "\n",
    "def filter_by_max_length(pt, en):    #判断长度是否<=40\n",
    "    return tf.logical_and(tf.size(pt) <=max_length, tf.size(en) <=max_length)\n",
    "    \n",
    "def tf_encode_to_subword(en_sentence, pt_sentence):    #进行pyfunction的封装\n",
    "    return tf.py_function(encode_to_subword,[en_sentence, pt_sentence], [tf.int64, tf.int64])\n",
    "    \n",
    "train_dataset = train_data.map(tf_encode_to_subword)\n",
    "train_dataset = train_dataset.filter(filter_by_max_length)     #舍弃长度大于40的数据\n",
    "train_dataset = train_dataset.shuffle(buffer_size).padded_batch(batch_size,padded_shapes= ([-1], [-1]))    #随机打乱数据， 将每64个样本补成相同长度\n",
    "\n",
    "val_dataset = val_data.map(tf_encode_to_subword)\n",
    "val_dataset = val_dataset.filter(filter_by_max_length)     #舍弃长度大于40的数据\n",
    "val_dataset = val_dataset.padded_batch(batch_size, padded_shapes= ([-1], [-1]))    #将每64个样本补成相同长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "26c9cea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 40) (64, 40)\n",
      "(64, 38) (64, 40)\n",
      "(64, 40) (64, 40)\n",
      "(64, 39) (64, 39)\n",
      "(64, 37) (64, 38)\n"
     ]
    }
   ],
   "source": [
    "for en,pt in val_dataset.take(5):\n",
    "    print(en.shape,pt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe48cd6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
